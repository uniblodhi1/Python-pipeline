# Python Data Analysis Pipeline

A sequential Python pipeline for processing cement production and emissions data.

## ğŸ“ Project Structure

```
Python-pipeline/
â”œâ”€â”€ data/                          # Input data files
â”‚   â””â”€â”€ Paper_Data.xlsx           # Main Excel data file
â”œâ”€â”€ outputs/                       # All output files (CSV, graphs, text)
â”‚   â””â”€â”€ (generated by scripts)
â”œâ”€â”€ scripts/                       # Python analysis scripts
â”‚   â”œâ”€â”€ 00_TEMPLATE.py            # Template for creating new scripts
â”‚   â”œâ”€â”€ 01_first_analysis.py      # Step 1: Read and inspect Excel data
â”‚   â”œâ”€â”€ 02_example_next_step.py   # Step 2: Example of pipeline connection
â”‚   â””â”€â”€ 03_..._22_.py             # Your 22 pipeline scripts
â””â”€â”€ README.md                      # This file
```

## ğŸ¯ How This Works (GitHub Basics)

### What is GitHub?
- Think of it as **Google Drive for code**
- Files stay where you put them (they don't move automatically)
- You can access your files from anywhere
- Others can see and use your code

### Key Concepts:
1. **Repository (Repo)**: Your project folder (this is your "Python-pipeline" repo)
2. **Commit**: Saving a snapshot of your changes
3. **Push**: Uploading your changes to GitHub
4. **Clone**: Downloading the repo to your computer

## ğŸš€ How to Use This Pipeline

### Step 1: Understanding the Flow

```
Excel File (data/) â†’ Script 01 â†’ Output 01 (outputs/) â†’ Script 02 â†’ Output 02 â†’ ... â†’ Script 22 â†’ Final Output
```

Each script:
1. **Reads** input (Excel file OR previous script's output)
2. **Processes** the data (your analysis)
3. **Saves** output (CSV, graph, text) in `outputs/` folder

### Step 2: Running Your Scripts

**Run scripts in order:**
```bash
python scripts/01_first_analysis.py
python scripts/02_example_next_step.py
python scripts/03_your_next_script.py
# ... and so on
```

**Important:**
- Run scripts ONE AT A TIME in numerical order
- Each script needs the previous script's output
- All outputs save to the `outputs/` folder automatically

### Step 3: Adding Your 22 Scripts

1. **Copy the template:**
   ```bash
   cp scripts/00_TEMPLATE.py scripts/03_your_analysis_name.py
   ```

2. **Edit the new script:**
   - Change `XX` in `INPUT_CSV` to the previous script number
   - Change `XX` in `OUTPUT_CSV` to your script number
   - Add your processing code in STEP 2

3. **Repeat for all scripts** (03, 04, 05, ... 22)

## ğŸ“ Example: Creating Script 03

```python
# In scripts/03_emissions_calc.py

# Configuration
INPUT_CSV = Path(__file__).parent.parent / "outputs" / "02_processed_results.csv"
OUTPUT_CSV = Path(__file__).parent.parent / "outputs" / "03_emissions_results.csv"

# Step 1: Read data from script 02
df = pd.read_csv(INPUT_CSV)

# Step 2: Your processing
df['total_emissions'] = df['scope1'] + df['scope2'] + df['scope3']

# Step 3: Save for script 04
df.to_csv(OUTPUT_CSV, index=False)
```

## ğŸ”§ How Files Connect

### The First Script (Special Case)
- **Input**: Excel file from `data/Paper_Data.xlsx`
- **Output**: CSV file to `outputs/01_outputs_historical_scopes.csv`

### All Other Scripts (Standard Pattern)
- **Input**: Previous script's CSV from `outputs/`
- **Output**: New CSV (or graph/text) to `outputs/`

### Path Example:
```python
# This works from anywhere in your project
INPUT_PATH = Path(__file__).parent.parent / "data" / "Paper_Data.xlsx"
# Breaks down to:
# __file__ = current script location
# .parent = go up to scripts/ folder
# .parent = go up to Python-pipeline/ folder
# / "data" / "Paper_Data.xlsx" = go into data folder and find file
```

## ğŸ“Š Output Files

All outputs go to the `outputs/` folder:
- **CSV files**: `01_name.csv`, `02_name.csv`, etc.
- **Graphs**: `01_graph.png`, `02_chart.png`, etc.
- **Text files**: `01_report.txt`, etc.

### Why this is good:
- âœ… Easy to find all results in one place
- âœ… Scripts can find each other's outputs
- âœ… You can download the entire `outputs/` folder from GitHub

## ğŸ’» Working with GitHub

### Uploading Your Scripts to GitHub:

```bash
# After creating/editing files:
git add scripts/03_your_new_script.py
git commit -m "Add script 03: emissions calculation"
git push
```

### Downloading Your Code:

```bash
# On a new computer:
git clone https://github.com/uniblodhi1/Python-pipeline.git
cd Python-pipeline
```

### Getting Updates:

```bash
# If you made changes on GitHub website:
git pull
```

## âš ï¸ Common Issues & Solutions

### Issue: "File not found"
**Solution**: Check that:
1. You ran the previous script first
2. The file name matches exactly
3. You're using the correct path pattern

### Issue: "No output created"
**Solution**:
1. Check if the script ran without errors
2. Look in the `outputs/` folder
3. Make sure your script has `df.to_csv(OUTPUT_CSV, ...)`

### Issue: "Can't find Paper_Data.xlsx"
**Solution**: Make sure it's in the `data/` folder and your script uses:
```python
INPUT_PATH = Path(__file__).parent.parent / "data" / "Paper_Data.xlsx"
```

## ğŸ“‹ Checklist for Your 22 Scripts

- [ ] Copy template for each script
- [ ] Number scripts in order (01, 02, 03, ... 22)
- [ ] Update INPUT_CSV to read from previous script
- [ ] Update OUTPUT_CSV with new name
- [ ] Add your analysis code
- [ ] Test by running scripts in order
- [ ] All outputs save to `outputs/` folder
- [ ] Commit and push to GitHub

## ğŸ“ Learning Resources

- **Git basics**: https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control
- **GitHub guide**: https://guides.github.com/activities/hello-world/
- **Python Path**: https://docs.python.org/3/library/pathlib.html

## ğŸ“§ Questions?

If something doesn't work:
1. Check error messages carefully
2. Verify file paths
3. Make sure you ran scripts in order
4. Check if previous outputs exist in `outputs/` folder

---

**Ready to start?** Begin by running `python scripts/01_first_analysis.py` and check the `outputs/` folder!
